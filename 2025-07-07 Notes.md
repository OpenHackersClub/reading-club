# 2025-07-07 OHC Reading Club Notes

## Background

### Materials

[12-Factor Agents: Patterns of reliable LLM applications â€” Dex Horthy, HumanLayer](https://www.youtube.com/watch?v=8kMaTybvDUw)

[12 Factor Agents](https://github.com/humanlayer/12-factor-agents)

## Discussion

### Overall Impression

The "12-factor AI agents" repository is considered high-level but provides valuable insights into the restrictions and experiences the author encountered when building AI agents. The "brief history of software" section was specifically highlighted as a most valuable piece of documentation for understanding the bottlenecks and the general overview of how agents should work.

### Outcomes

#### Criticisms and Limitations

- Lack of Implementation Detail: The documents are criticized for being too high-level and not providing specific details on how to implement certain aspects.
- Missing Minimum Credentialed Principle (MCP)
- Limited Focus on Streaming: There's a perceived lack of focus on real-time streaming capabilities, which are increasingly important for agent performance and challenging in DAG frameworks.
- Insufficient Eval Workflows (Evals): A significant gap identified is the lack of guidance on how to build evaluation workflows.
- Generality and Abstraction: Some factors are seen as too abstract, making their practical optimization goals hard to grasp. The document feels more like a structured blog post or experience sharing rather than a systematic framework or guideline for framework to follow.

#### Comparisons

- Google's Agent White Paper: This is suggested as a more structured and systematic resource, categorizing agent patterns and offering better definitions for human interaction and tool integration nuances, including security considerations (e.g., agent providing responses but the client handling API calls with credentials).
- OpenAI Agent SDK: This SDK is noted for already covering most of the "12 factors" by providing concrete "how-to" examples and practices for elements like human interruption, orchestration, prompt injection, and input/output handling. While the "12 factors" explain the "why," the OpenAI SDK shows the "how".
- The N8N community generally aligns with principles like building agents in modular components and structured outputs, allowing for chaining workflows and avoiding monolithic agents.

### Next Steps for the Reading Club

The club decided to evolve its format and focus to deepen their understanding of AI agents:

#### Adjusted Meeting Format

- Weekly Reading & Discussion (Short-Term Exploration):
  - Continue weekly sessions for reading and discussion, but with a more structured approach. The first session of each month could be more exploratory, "lighting talk" style, where participants share new discoveries or broad topics to gauge community interest and encourage learning. This aims to provide everyone with something new to hear.
- Monthly Deep Dives (Top-Weighted Topics):
  - Subsequent sessions each month will focus on "top-weighted" topics, potentially inviting external speakers for more in-depth knowledge sharing and aggregating high-quality, deep-dive content. This rhythm balances weekly cadence with deeper, less frequent dives.
- Key Topics for Future Exploration:
  - Evals (Evaluation) of AI Agents
  - Orchestration: Further exploration into how agents are orchestrated.
  - Human Interruption: A topic of continued interest regarding human-in-the-loop processes.

- Structured Learning Approach:
  - Book Reading for Common Language:
    - Consider reading a foundational book, such as one on MLOps or Designing Machine Learning Systems, to establish a common language and framework for discussions and categorization. This could involve an exploratory session to decide on a book.
    - Divide and Conquer:
      - Participants will divide and conquer reading and research tasks for efficiency, sharing notes and talking points. This allows individuals to leverage their current work interests and learn more effectively. For instance, one person might focus on a book chapter for general understanding, while another deep dives into specific research papers or practical implementations related to a particular topic (e.g., evals).
    - Integration with Personal Projects:
      - Encourage integrating learned concepts into personal hobby projects and sharing progress weekly, fostering practical application.

## Extra Notes from participants

[Agents in a finite state machine](https://x.com/debuggingfuture/status/1942411514251776218)
