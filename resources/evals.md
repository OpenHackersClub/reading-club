# Evals - AI Evaluation Resources

A curated collection of materials for understanding and implementing evaluation frameworks for AI systems, particularly focusing on LLMs and AI agents.

## Platforms & Tools

### Open Source Platforms

- [ ] [Opik by Comet ML](https://github.com/comet-ml/opik) - Open-source evaluation framework for LLMs
- [ ] [Phoenix by Arize AI](https://github.com/Arize-ai/phoenix) - LLM observability and evaluation
- [ ] [Langfuse](https://langfuse.com/) - Open source LLM engineering platform for tracing, evaluations, and observability

## Research Papers & Academic Resources

### Survey Papers

- [ ] [Survey on Evaluaton of LLM-based Agents](https://arxiv.org/pdf/2503.16416)
