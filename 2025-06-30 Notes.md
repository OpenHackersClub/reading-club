# Background
## Materials
[Practice For Governing Agentic AI System](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)
[Entra Agent ID](https://techcommunity.microsoft.com/blog/microsoft-entra-blog/announcing-microsoft-entra-agent-id-secure-and-manage-your-ai-agents/3827392)
[OpenAI Agents Python](https://github.com/openai/openai-agents-python)

# Discussion
## Intro of the paper
The OHC Reading Club recently convened to discuss a paper titled "Practices for Governing Agentic AI Systems," focusing on the challenges and potential solutions for controlling and ensuring the safety of AI agents. The authors of the paper intentionally kept it abstract due to the rapid evolution of the AI landscape, aiming to offer open-ended questions and important characteristics rather than definitive best practices.

## Outcomes
We mainly discussed the open-ended questions and challenges mentioned in the paper (Section 4 of the paper). You can find it in the section "Appendix" of this note as well.

-  **Evaluating Suitability for the Task**:
    - One suggested approach from Open AI agent Python SDK is to **use another Large Language Model (LLM) as a judge** to evaluate if an agent's output is acceptable or makes sense before proceeding to the next step.
    - This concept was applied in a hackathon, where agents would fact-check and comment on proposals, similar to digital twins or opinion leaders on platforms like Twitter, before publication.
    - A significant challenge noted is the **potential for the judging or monitoring LLM to hallucinate** itself, as agents are not deterministic.
    - Solutions discussed involve making the process **transparent and verifiable**, akin to crypto perspectives where every piece of information in the supply chain is verifiable. This is being actively developed, with examples in scientific research for quoting papers or experiments.
    - A recent news report highlighted academics intentionally using "prompt injection" within their papers to prevent AI agents from misbehaving when checking bad data, demonstrating the existing problem and the need for verification checks across the supply chain.

- **Constraining the Action Space and Requiring Approval**:
    - This is crucial for preventing harm and maintaining control over agents.
    - A significant challenge is **"Agent Sprawl,"** where users invite numerous agents (e.g., translation, transcription, summarisation agents) into an ecosystem, leading to a massive increase in the number of "users" (agents) to manage, creating a security headache.
    - **Entra Agent ID** (from Microsoft) is an industry-first attempt to address this by provisioning agent identities and integrating them back into existing Identity and Access Management (IAM) systems like Azure AD (referred to as "on ID"). Solutions like Auth0 are also working to automatically integrate spawned agent IDs into their ecosystems for a single view of provisioned identities.
    - A core problem is determining whether an agent should have the same permissions as its human user or only a subset. Solutions are emerging that use **OAuth scopes** to bind specific permissions to API calls, allowing agents to have different OAuth tokens and scopes than their human counterparts.
    - The use of **decentralised IDs and crypto-based authentication** is proposed as a better fit for the AI agent-empowered world, allowing policy engines to run access based on on-chain reputation for a scalable access control system, regardless of the number of agents.
    - The overall challenge stems from developers giving LLMs too much access to data or tools, making it harder to restrict their actions compared to traditional role-based APIs.

- **Setting Agents' Default Behaviors**:    
    - System prompts can be highly effective in influencing an LLM's default behavior. For example, explicitly instructing an LLM to state if it cannot answer a question instead of fabricating a response, or to only use provided data rather than its own trained knowledge, can significantly increase the success rate of desired outcomes and prevent misleading answers.

- **Legibility of Agent Activity**:
    - The ability to trace the entire route of an agent's operations is crucial for debugging and understanding how agents collaborate and hand off contexts. The Open AI agent Python SDK provides this tracing capability.

- **Automatic Monitoring**:
    - While the concept of an external system reviewing an agent's reasoning and actions exists, participants noted a lack of useful or mature solutions in this area, sometimes perceiving it as "noise".
    - However, specific use cases, such as monitoring financial trading agents for risk ratios or asset limits, could benefit from such monitoring, though working solutions are not yet mature.
    - "Deep thinking models" that self-correct their answers were mentioned as a somewhat similar idea in terms of reviewing reasoning, demonstrating progress in agents' internal self-correction.

- **Attributability**:
    - This is linked to agent identity. Once an agent ID is integrated into an IAM system, it allows for **audit trail capabilities**, similar to how AWS IAM logs server API key usage, making agent actions attributable.
    - **On-chain attestations** were highlighted as particularly relevant for tagging agents and linking their actions, especially in blockchain environments where agents are essentially users.

# Up Next?
- Weekly Reading Club on Monday 22:00 GMT+8.
- The topics will be focusing on AI, Local-first
- Format of reading club will be either:
	- Select and read materials (papers/articles/videos) before the meet-up. Share learnings, thoughts and questions in the meetup, **OR**
	- Crowdsource a topic. 2-3 of the participants researching on that topic, and share on the meet-up
- The topic of the next meet-up should be voted and selected by Wednesday so everyone got enough time to prepare.

# Appendix
## Open-end questions in the paper
### General Open Question

- **What harm mitigations, if any, are primarily attainable via technical choices in the model’s training process? What might corresponding best practices be?**

### Evaluating Suitability for the Task

This practice involves assessing whether a given AI model and agentic system are appropriate for their intended use case and can reliably execute tasks.

- **How can system deployers and users effectively evaluate the agentic system’s level of reliability in their use case? What constitutes “sufficient” evaluation?**
- **How can system deployers effectively evaluate the combination of agent and user, and identify behaviors and potential failures that only emerge through human-agent interaction?**
- **Given the heterogeneous nature of real-world deployment, what failure modes cannot be expected to be detected in advance via evaluation?**
- **What evaluations of agents’ capabilities should be expected to be done by the model developer, rather than the system deployer?** (E.g. universally useful checks, such as the system’s propensity to act in alignment with the user’s goals.)
- **How can system deployers communicate to the user the intended conditions under which the agentic system can be used reliably, and at what point does a user’s unintended usage of a system make them responsible for resulting harms?**
- **What misusable agentic system capabilities should model developers and system deployers be obligated to test for, both for specific sectors and for agents in general?**
### Constraining the Action-Space and Requiring Approval

This practice focuses on limiting actions an agent can take and requiring human authorization for critical decisions, often referred to as keeping a "human-in-the-loop".

- **How should a user or system deployer determine and enforce which interactions need human approval, as a function of a model’s empirical reliability on that task and other factors?** Are there certain interactions, such as formation of legal contracts, where a user’s approval should always be required?
- **What design practices should system deployers follow to ensure users have sufficient context before approving an action?**
- **What are the best practices for users reviewing approvals for high-cost actions (such as minimum review times) to avoid their turning into a “rubber stamp” for agentic AI systems that cannot catch harmful actions?**
- **What are the best practices for system deployers to test that agentic AI systems follow the action constraints the system deployer specified?**
- **How can the effectiveness and meaningfulness of human oversight be evaluated?**
- **How can existing sandboxing mechanisms be modified to work for agentic AI systems?** How can model developers or system deployers test the reliability of a sandbox for a given agentic system without incurring the actual risk of the agentic system breaking that sandbox, or a wider nested sandbox test environment?

### Setting Agents’ Default Behaviors

This practice involves proactively shaping the models' default behavior with common-sense preferences to mitigate accidental harm, even when user preferences are unclear.

- **What other default behaviors could model developers and system deployers instill in agentic AI systems that could mitigate the possibility of errors and harms?**
- **How should these default behaviors be balanced, when in conflict?**
- **How is responsibility allocated between the model developer (who may not have intended for their model to be used in a particular agentic system) and the system deployer, when it comes to instilling certain behaviors in AI systems?**

### Legibility of Agent Activity

This practice emphasizes making agents' actions and internal reasoning transparent to users to facilitate error detection, debugging, and trust.

- **Should system deployers be required to provide their agents’ “internal monologue” to the user?**
- **Should all AI agent reasoning, including inter-agent communication, be required to occur in natural language, in order to make subsequent auditing by humans easier?**
- **What methods can model developers and system deployers take in order to produce accounts of agents’ thoughts that are more faithful to the logic the system truly employed?**
- **How can model developers make their models reason in ways that are more legible to the user or system deployer?**
- **What user-experience practices should system deployers follow in order for users to be able to easily understand and act on agents’ internal reasoning?**
- **How can users easily identify the actions that are most likely to be mistaken or ill-informed, among many actions taken by their agent?**

### Automatic Monitoring

This practice suggests setting up a second "monitoring" AI system to automatically review the primary agent's reasoning and actions, operating at a speed and cost that human monitoring cannot match.

- **How can system deployers verify that a monitor is capable of successfully rooting out mistakes from the primary AI system, particularly when they may have been trained in a similar manner, or when the agent is substantially more intelligent than the monitor?**
- **What types of human oversight of a primary AI system may still be useful when AI-based monitoring is also in use?**
- **What key behaviors should monitoring systems monitor for that would otherwise break implicit safety practices, such as agents undermining a user’s “blank slate” assumption by storing information somewhere on the internet so that they can retrieve it in a future session?**
- **How can automatic monitoring systems be made to monitor for as-yet-unknown harms?**

### Attributability

This practice aims to deter harm by making it likely that a user's actions, especially in high-stakes interactions, would be traced back to them, perhaps through unique agent identifiers.

- **How can society practically enable AI agent identity verification?** What existing systems, such as internet certificate authorities, can be adapted to facilitate such verification?
- **What other ideas exist for practically enabling agentic AI system attributability?**

### Interruptibility and Maintaining Control

This critical backstop practice ensures the ability to gracefully shut down an agent at any time to prevent accidental or intentional harm.

- **How can model developers and system deployers design their systems to ensure that agentic systems have graceful fallbacks in case they’re shut down or interrupted, for the broad range of actions an agent might take?** Are there principles by which a second agentic AI system could be used as the fallback, and where might this approach fail?
- **In what settings is interruptibility users’ responsibility, rather than model developers’ or system deployers’?** For instance, should users be considered responsible for only approving an agent’s action if it is coupled with a fallback procedure?
- **How can system deployers ensure that agents only spawn sub-agents that can be similarly turned off?**
- **Under what circumstances, if any, should an agent ever be able to (or be incentivized to) prevent its own termination?**
- **What information should system deployers or compute providers keep track of (such as agent IDs, as in Section 4.6), in order to help determine that a system they’re hosting has caused significant harm, and needs to be turned off?** How can such information be minimized to satisfy the strong need for user privacy?
- **What restrictions should exist on such shutdowns, to prevent them from being abused to police harmless or low-stakes usage of agents?**
- **How realistic is it for agentic AI systems to resist being shut down in the near-term?** How realistic is it for an agentic AI system to be integrated into a social process or critical infrastructure (including unintentionally) such that the cost of shutting it down would become prohibitive? If either scenario did happen, what are the likeliest pathways, and what signals might be observed in the run-up (by the system deployer and user, or by outside parties) that can be used to trigger intervention ahead of time?
- **How should different parties’ responsibilities be allocated in the event of the non-interruptibility of an AI system that causes harm?**